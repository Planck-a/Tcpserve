`六大高并发模型：多进程、多线程、线程池、select、poll、epoll`

1、多进程并发编程
---
利用的是TCP协议
核心思想：一个服务器和客户端进行并发访问，每个客户端都有自己的套接字描述符，在connect()、send()、recv()时发送对应的socket文件描述符
(就知道是哪个客户端跟服务器进行通信，谁发的数据，将数据发给谁就一清二楚)；然后服务器给每一个客户端都fork()一个子进程，形成一对一的访问机制；
当一个客户端连接关闭时，的退出当前子进程
缺点：a、启动和关闭子进程带来很大的开销；b、系统最多只能产生512个进程，也就是说最多只有512个客户，形成不了处理大型访问的情形

2、多线程并发编程
---
利用的是TCP协议
每到来一个客户端，就创建一个线程，通过其中的函数指针来处理业务；通过connect()来进行连接，发送其客户端的套接字；
是一对一的服务；当一个客户端连接关闭时，的退出当前子线程
缺点：多线程网络服务也存在线程的动态申请与释放，还是有一定的开销，若存在大量用户在线，很可能带来线程间切换开销

3、线程池并发编程
---
针对多线程网络服务模式的一些不足之处而提出的改进模式
其基本理念是：先创建一批资源，当有用户到来时，直接分配以创建好的资源，它的主要目的是减少系统在频繁创建资源时的开销实现原理：
主服务线程创建既定数量的服务线程，当有客户端到来时，则从线程池中找出空闲的服务线程，为其服务，服务完毕后，
线程不进行释放，重新放回线程池；若当前线程池已满，则将当前的客户端加入等待队列

设计线程池的思路：主线程中创建一个池子，池子中包含了要处理的任务队列，加任务操作是主线程在中进行的。如果任务队列为空，那么子线程就会调用condition_timedwait()阻塞，同时释放自己的锁，让其他线程访问池子；如果主线程现在有任务要处理，就会调用condition_signal()唤醒其中一个子线程，去处理任务。

任务列表
```cpp
typedef struct task
{
    void *(*run)(void *args);//函数指针
    task *pnext;
}task;
```
线程池设计
```cpp
typedef Threadpool
{
    condition_t ready;    //状态量，池子的控制权
	task_t *first;       //任务队列中第一个任务
	task_t *last;        //
	int counter;         //线程池中已有线程数
	int idle;            //线程池中空闲的线程数
	int max_threads;     //线程池最大线程数
	int quit;            //是否退出标志
}Threadpool;
```
线程池加入任务
 1、将任务加入到任务队列，加入到链表最后位置
 2、判断池子的情况，接收第一个任务时，池子是空的，所以会先创建新线程；之后再有任务，判断，如果有线程闲，则条件变量激活；如果没线程空闲，但是还不到最大线程数，创建新线程
 ```cpp
 产生新任务，加入到队列的尾部
condition_lock(&pool->ready);

//线程池中有线程空闲，唤醒
if (pool->idle > 0)
	{
		condition_signal(&pool->ready);
	}
 //虽然没有线程空闲，但是池中线程个数没有达到设定的最大值，可以创建一个新的线性
 else if (pool->counter < pool->max_threads)
	{
		pthread_t tid;
		pthread_create(&tid, NULL, thread_routine, pool);
  //创建新线程，执行thread_routine，参数是pool
		pool->counter++;
	}
	//结束，访问
	condition_unlock(&pool->ready);
 ```
 创建新线程
 ```cpp
 while (1)
{
    condition_lock(&pool->ready);//加锁就表明我在向池子申请任务
    while (池子任务列表为空)
    {
        condition_timedwait();//阻塞，等待被唤醒
        //阻塞时会释放锁，允许其他线程访问，当被唤醒时，自动加锁
    }
    if(队列不为空)
    {
        //取出等待队列最前的任务，移除任务，并执行任务
        //由于任务执行需要消耗时间，先解锁让其他线程访问线程池
        condition_unlock(&pool->ready);
        task->run(args);
        condition_lock(&pool->ready);
        //重新加锁，加锁就表明我在向池子申请任务
    }
}
 ```

`补充:`
linux下条件变量 ------>条件变量要和锁一起使用<---------windows下条件变量
```cpp
pthread_mutex_lock
pthread_mutex_unlock
pthread_cond_wait
pthread_cond_timewait ------>如果很长时间都没有激活，子线程阻塞一定后就退出
pthread_cond_signal
pthread_cond_broadcast

conditiona_variable -cv;
_cv.notify_one();
```


4、IO多路复用
---
Linux I/O多路复用，都是针对高并发的情况下，创建一个线程可以为多个客户服务的一种模式；同一个时刻，只能为一个客户服务(作用排队)

* select()：
select的调用在FD_ZERO()和FD_SET()之后调用

* poll()：
poll()的调用时机和select()一样，在监听之后

* epoll()：
epoll的调用顺序：socket---->bind----->listen----->epoll_create----->增加事件(epoll_ctl())>epoll_wait
----->accept(建立连接)---->读/写(在读写中删除和修改事件-->通过epoll_ctl())

分模块的执行每一块代码:

epoll_create()；创建一个文件描述符，来唯一的标识内核中创建的事件表(用户关心的文件描述符就放在内核事件表中)

epoll_ctl()：往事件表上注册/修改/删除事件

epoll_wait()：返回就绪的文件描述符的个数，如果检测到事件，就将所有就绪的事件从内核事件表中复制到它的第二个参数events指向的数组中
该events数组只输出检测到的就绪事件


epoll的工作模式：

LT(电平触发)：当epoll_wait检测到其上有事件发生并将此事件通知应用程序后，应用程序可以不立即处理该事件，这样，当下一次调用epoll_wait时，还会再次向应用程序告知此事件，直到该事件被处理

ET(边沿触发)：当epoll_wait检测到其上有事件发生并将此事件通知应用程序后，应用程序必须立即处理该事件
所以：ET模式在很大程度上降低了同一个epoll事件被重复触发的次数，因此效率要比LT模式高

EPOLLONESHOT事件：针对使用ET模式还是可能被触发多次，只有在epoll_ctl函数的文件描述符上注册EPOLLONESHOT事件，此时只触发一次

`三组I/O复用函数的比较`
都能同时监听多个文件描述符，直到一个或多个文件描述符上有事件发生时返回，返回值是就绪的文件描述符的数量事件集：
select：参数fd_set，没有将文件描述符和事件绑定；其仅仅是一个文件描述符集合
poll：参数pollfd，将文件描述符和事件都定义其中，任何事件都被统一处理，使得编程接口简洁
select和poll每次调用都返回整个用户注册的事件集合(就绪的和未就绪的)；索引就绪文件描述符的时间复杂度：O(n)
epoll：在内核中维护一个事件表，通过epoll_ctl往其中添加、删除、修改事件，epoll_wait都直接从内核事件表中取得用户注册事件，而不用反复从用户空间
读入事件，索引就绪文件描述符的时间复杂度：O(1)

`最大支持文件数：`
select ：1024
poll、epoll：系统打开最大文件描述符的数目；65535

`工作模式：`
select和poll只能工作在相对低效的LT模式，
epoll可以工作在ET高效模式，epoll还支持EPOLLONESHOT事件，该事件进一步减少事件被触发次数；

`具体实现：`

select和poll：采用轮询方式，每次调用都要扫描整个注册文件描述符
epoll_wait：无需轮询整个文件描述符，根据下标直接定位(events数组中即就绪文件)，算法复杂度：O(1)
当活动连接比较多的时候，epoll_wait的效率未必比select和poll高，因为此时回调函数被触发的过于频繁，epoll_wait适用于连接数量多，但活动连接少的情况

select()和poll()模式主要有3个缺点：
(1)、select()的fd有最大的限制，1024/2048，并发数被限制了，poll的并发数与epoll相同
(2)、内存拷贝问题，每次都需要将fd集合从用户态拷贝到内核态，开销较大，内存拷贝方法内核把fd通知给用户空间
(3)、select模式每次都会线性的扫描从内核传递进来的fd集合，效率比较低
引出了epoll模型，epoll模型是Linux独有的I/O复用模型，做了很大的改进
(1)、epoll模型没有最大并发连接的限制，所可以并发的数目很系统内存有很大的关系
(2)、epoll模型使用了"共享内存"，不存在内存拷贝的问题了
(3)、epoll模型只管连接"就绪"的，而跟连接总数无关，也就不存在遍历了
```cpp
epoll模型的高效关键在于数据结构的设计
typedef union epoll_data {
 void        *ptr;
 int          fd;  //一般情况下，都用的是这个文件描述符
 uint32_t     u32;
 uint64_t     u64;
} epoll_data_t;


struct epoll_event {
   uint32_t     events;      /* Epoll events */
   epoll_data_t data;        /* User data variable */
};
```
epoll_data是一个union结构体，保存了：fd，指针，利用其就可以直接定位目标了
epoll_wait()
对epoll_wait()函数的核心理解：
(1)、返回值：事件表中就绪客户端的个数；等待队列
(2)、参数events：将事件表中的就绪客户端的信息放到了events数组中。
epoll()的核心思想
就是创建一个内核事件表，存放所监听客户端的套接字和当前的事件，在利用epoll_wait()函数查找就绪的套接字，最后经过增加、删除、修改利用
epoll_ctl()函数进行
